
# ğŸ§ ğŸ“š Alice LLM Lab  
*A Tiny Transformer Language Model with Retrieval-Augmented Generation*

![Python](https://img.shields.io/badge/Python-3.x-3776AB?logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-Used-EE4C2C?logo=pytorch&logoColor=white)
![Streamlit](https://img.shields.io/badge/Streamlit-UI-FF4B4B?logo=streamlit&logoColor=white)
![SQLite](https://img.shields.io/badge/SQLite-Database-003B57?logo=sqlite&logoColor=white)
![Status](https://img.shields.io/badge/Status-Learning_Project-FBC02D)

---

## ğŸ“Œ Project Overview

Alice LLM Lab is a custom transformer-based language model trained on *Aliceâ€™s Adventures in Wonderland*.  
This project was built as a hands-on learning exercise to understand how language models work end to end, starting from raw text preparation, moving through training a transformer from scratch, and extending generation using retrieval-based context.

The model operates at a character level, uses explicit training and validation splits, and saves checkpoints and loss plots during training so learning behavior is visible.  
To keep generated responses grounded, processed text is chunked and stored in a local SQLite database, allowing relevant passages to be retrieved and injected into prompts when needed.

Everything runs locally and the focus is on clarity, experimentation, and understanding rather than production scale optimization.

---

## ğŸ“ What this project is

Alice LLM Lab is a learning-focused project that lets you:

â€¢ Train a small transformer language model from raw text  
â€¢ Observe how training progresses through checkpoints and loss curves  
â€¢ Generate text in the style of the training corpus  
â€¢ Experiment with retrieval-augmented generation using local data  
â€¢ Interact with the model through a simple Streamlit interface  

> â„¹ï¸ **Note**  
> The model is trained from scratch on the provided dataset and runs entirely on your local machine. No external model APIs are used.

---

## ğŸ§  How the system works

At a high level, the system follows a simple and traceable flow:

1. Raw text is cleaned and normalized  
2. The processed text is split into training and validation data  
3. Text is chunked and stored in SQLite for retrieval  
4. A character-level transformer is trained using PyTorch  
5. Relevant text chunks are retrieved using TF-IDF similarity  
6. Retrieved context is optionally combined with prompts during generation  

---

## ğŸ”— How the programs are connected

The pipeline begins with dataset preparation, where raw text is cleaned, split, chunked, and stored in a SQLite database.  
The training module then consumes this prepared data and produces model checkpoints and loss visualizations.

For inference, standard generation loads the trained model directly, while retrieval-augmented generation first searches the SQLite database for relevant context before generating text.  
The Streamlit application provides a single interface that ties these components together for interactive experimentation.

---

## ğŸ“¤ Outputs generated

Running the project produces:

â€¢ Cleaned and processed text files  
â€¢ Chunked text stored in SQLite  
â€¢ Trained model checkpoints  
â€¢ Training loss plot  
â€¢ Generated text samples  

---

## ğŸ—‚ï¸ Project structure

```
alice-mini-llm/
â”‚
â”œâ”€â”€ app/                         # Application layer
â”‚   â””â”€â”€ streamlit_app.py         # Streamlit UI for interactive inference
â”‚
â”œâ”€â”€ data/                        # Dataset storage
â”‚   â”œâ”€â”€ raw/                     # Original input data
â”‚   â”‚   â””â”€â”€ alice.txt            # Raw Alice in Wonderland text
â”‚   â”‚
â”‚   â”œâ”€â”€ processed/               # Cleaned and prepared data
â”‚   â”‚   â”œâ”€â”€ alice_clean.txt      # Normalized text after cleaning
â”‚   â”‚   â”œâ”€â”€ chunks.jsonl         # Text chunks used for retrieval
â”‚   â”‚   â”œâ”€â”€ train.txt            # Training split
â”‚   â”‚   â””â”€â”€ val.txt              # Validation split
â”‚   â”‚
â”‚   â””â”€â”€ texts.db                 # SQLite database for retrieval
â”‚
â”œâ”€â”€ outputs/                     # Generated artifacts
â”‚   â”œâ”€â”€ checkpoints/             # Saved model checkpoints
â”‚   â”‚   â”œâ”€â”€ model.pt             # Final trained model
â”‚   â”‚   â”œâ”€â”€ model_best.pt        # Best performing checkpoint
â”‚   â”‚   â””â”€â”€ model_latest.pt      # Most recent checkpoint
â”‚   â”‚
â”‚   â””â”€â”€ plots/                   # Training visualizations
â”‚       â””â”€â”€ loss.png             # Training loss curve
â”‚
â”œâ”€â”€ src/                         # Core source code
â”‚   â”œâ”€â”€ data_prep/               # Dataset preparation logic
â”‚   â”œâ”€â”€ model/                   # Transformer, training, generation code
â”‚   â”œâ”€â”€ rag/                     # Retrieval and RAG logic
â”‚   â”œâ”€â”€ eval/                    # Evaluation utilities
â”‚   â”œâ”€â”€ config.py                # Central configuration
â”‚   â””â”€â”€ inference.py             # Shared inference helpers
â”‚
â”œâ”€â”€ Execution_Guide.md            # Step-by-step execution guide
â”œâ”€â”€ Project_Report.md             # Detailed project explanation
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ pyproject.toml                # Project configuration
```

---

## âš™ï¸ Setup and execution

Create a virtual environment:
```
python -m venv .venv
```

Activate it:

Windows (PowerShell)
```
.venv\Scripts\Activate.ps1
```

macOS / Linux
```
source .venv/bin/activate
```

Install dependencies:
```
pip install -r requirements.txt
```

Prepare the dataset:
```
python src/data_prep/dataset_builder.py
```

Train the model:
```
python src/model/train.py
```

Generate text:
```
python src/model/generate.py --prompt "Alice was beginning to"
```

Run the Streamlit app:
```
streamlit run app/streamlit_app.py
```

---

## ğŸ¥ Demo video

You can add a short demo video here showing the dataset preparation, training process, and interactive generation.

Demo link:
```
<replace this with your demo video link>
```

---

## âš ï¸ Notes and limitations

â€¢ Learning-focused prototype  
â€¢ Small model trained on limited data  
â€¢ Retrieval quality depends on chunking and TF-IDF similarity  
â€¢ Performance depends on local hardware  

---

## ğŸ™Œ Author

**Abinash Prasana Selvanathan**  

â­ If you found this project useful, feel free to star the repository.
