
# ğŸ§ ğŸ“š Alice LLM Lab
*A Tiny Transformer Language Model with Retrieval-Augmented Generation*

![Python](https://img.shields.io/badge/Python-3.x-3776AB?logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-Used-EE4C2C?logo=pytorch&logoColor=white)
![Streamlit](https://img.shields.io/badge/Streamlit-UI-FF4B4B?logo=streamlit&logoColor=white)
![SQLite](https://img.shields.io/badge/SQLite-Database-003B57?logo=sqlite&logoColor=white)
![ML](https://img.shields.io/badge/ML-Transformer_Model-4CAF50)
![Status](https://img.shields.io/badge/Status-Learning_Project-FBC02D)

---

## ğŸ“Œ Project Overview

Alice LLM Lab is a **custom transformer-based language model** trained on *Aliceâ€™s Adventures in Wonderland*.  
The project was built as a **hands-on learning exercise** to understand how modern language models work internally, starting from raw text preprocessing, moving through model training, and finally extending generation with retrieval-based context.

Everything runs locally and the emphasis is on **learning core concepts clearly**, not on production-scale optimization.

---

## ğŸ¯ What this project covers

This project focuses on understanding and implementing the following core ideas:

ğŸ“– Character-level language modeling  
ğŸ§  Transformer architecture built from scratch  
ğŸ‹ï¸ Model training and checkpointing  
âœï¸ Text generation with temperature control  
ğŸ§© Retrieval-Augmented Generation using local data  
ğŸ—„ï¸ SQL-based storage for text retrieval  
ğŸ–¥ï¸ Interactive inference using Streamlit  

Each part is implemented explicitly so the full pipeline is easy to trace and reason about.

---

## ğŸ§  Techniques and methods used

The project intentionally uses a small but important set of techniques that appear in real-world LLM systems:

ğŸ”¹ **Custom Transformer Architecture**  
Multi-head self-attention and feed-forward layers implemented using PyTorch.

ğŸ”¹ **Character-Level Tokenization**  
The model learns directly from raw characters rather than prebuilt tokenizers.

ğŸ”¹ **Train / Validation Split**  
Clean separation of training and validation data to track learning behavior.

ğŸ”¹ **Model Checkpointing**  
Best, latest, and final model checkpoints are saved during training.

ğŸ”¹ **Loss Visualization**  
Training loss is tracked and plotted to understand convergence.

ğŸ”¹ **Text Chunking**  
Processed text is split into overlapping chunks for retrieval.

ğŸ”¹ **SQLite Database**  
All text chunks are stored in a local SQLite database for fast lookup.

ğŸ”¹ **TF-IDF Similarity Search**  
Relevant text chunks are retrieved using TF-IDF based similarity scoring.

ğŸ”¹ **Retrieval-Augmented Generation**  
Retrieved context is injected into the prompt to guide generation.

ğŸ”¹ **Streamlit Interface**  
A lightweight UI for interactive prompting and experimentation.

---

## ğŸ—‚ï¸ Project Structure (explained)

```
alice-mini-llm/
â”‚
â”œâ”€â”€ app/
â”‚   â””â”€â”€ streamlit_app.py        # Streamlit UI for inference
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ alice.txt           # Original dataset
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ alice_clean.txt     # Cleaned text
â”‚   â”‚   â”œâ”€â”€ chunks.jsonl        # Chunked text for retrieval
â”‚   â”‚   â”œâ”€â”€ train.txt           # Training data
â”‚   â”‚   â””â”€â”€ val.txt             # Validation data
â”‚   â””â”€â”€ texts.db                # SQLite database
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ checkpoints/            # Model checkpoints
â”‚   â””â”€â”€ plots/
â”‚       â””â”€â”€ loss.png            # Training loss curve
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_prep/              # Dataset preparation
â”‚   â”œâ”€â”€ model/                  # Transformer, training, generation
â”‚   â”œâ”€â”€ rag/                    # Retrieval logic
â”‚   â”œâ”€â”€ eval/                   # Metrics
â”‚   â”œâ”€â”€ config.py               # Configuration
â”‚   â””â”€â”€ inference.py            # Shared inference helpers
â”‚
â”œâ”€â”€ Execution_Guide.md
â”œâ”€â”€ Project_Report.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ pyproject.toml
```

---

## âš™ï¸ Setup

### 1ï¸âƒ£ Create a virtual environment

```
python -m venv .venv
```

Activate it:

**Windows (PowerShell)**

```
.venv\Scripts\Activate.ps1
```

**macOS / Linux**

```
source .venv/bin/activate
```

---

### 2ï¸âƒ£ Install dependencies

```
pip install -r requirements.txt
```

---

## ğŸ§ª Dataset Preparation

```
python src/data_prep/dataset_builder.py
```

This step cleans the text, creates chunks, builds the SQLite database, and prepares training files.

---

## ğŸ‹ï¸ Model Training

```
python src/model/train.py
```

Outputs include model checkpoints and a training loss plot.

---

## âœ¨ Text Generation

**Standard generation**

```
python src/model/generate.py --prompt "Alice was beginning to"
```

**Retrieval-augmented generation**

```
python src/rag/rag_generate.py --prompt "Who is the Queen of Hearts?" --top_k 3
```

---

## ğŸ–¥ï¸ Streamlit App

```
streamlit run app/streamlit_app.py
```

Use the UI to experiment with prompts and retrieval settings.

---

## âš ï¸ Notes and limitations

âœ”ï¸ Learning-focused prototype  
âœ”ï¸ Small model trained on limited data  
âœ”ï¸ Retrieval quality depends on chunking and TF-IDF similarity  
âœ”ï¸ Performance depends on local hardware  

---

## ğŸ™Œ Author

**Abinash Prasana Selvanathan**  

â­ If you found this project useful, feel free to star the repository.
